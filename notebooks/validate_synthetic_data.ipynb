{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import composuite\n",
    "from diffusion.utils import *\n",
    "from diffusion.utils import *\n",
    "from collections import defaultdict\n",
    "import composuite\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def split_dict(data, split_ratio=0.7):\n",
    "\n",
    "    train_dict, test_dict = {}, {}\n",
    "    for key, values in data.items():\n",
    "        values = np.array(values)  # Ensure it's an array\n",
    "        indices = np.random.permutation(len(values))\n",
    "        split_idx = int(len(values) * split_ratio)\n",
    "        train_dict[key] = values[indices[:split_idx]]\n",
    "        test_dict[key] = values[indices[split_idx:]]\n",
    "\n",
    "    return train_dict, test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = 'IIWA'\n",
    "obj = 'Dumbbell'\n",
    "obst = 'ObjectDoor'\n",
    "subtask = 'Trashcan'\n",
    "\n",
    "representative_indicators_env = composuite.make(robot, obj, obst, subtask, use_task_id_obs=True, ignore_done=False)\n",
    "modality_dims = representative_indicators_env.modality_dims\n",
    "\n",
    "\n",
    "base_agent_data_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/data'\n",
    "dataset = load_single_composuite_dataset(base_path=base_agent_data_path, \n",
    "                                            dataset_type='expert', \n",
    "                                            robot=robot, obj=obj, \n",
    "                                            obst=obst, task=subtask)\n",
    "agent_dataset = transitions_dataset(dataset)\n",
    "agent_dataset, _ = remove_indicator_vectors(modality_dims, agent_dataset)\n",
    "\n",
    "base_synthetic_data_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/cluster_results/diffusion/cond_diff_20/train/'\n",
    "synthetic_dataset = load_single_synthetic_dataset(base_path=base_synthetic_data_path, \n",
    "                                                  robot=robot, obj=obj, \n",
    "                                                  obst=obst, task=subtask)\n",
    "\n",
    "print(agent_dataset['observations'].shape, synthetic_dataset['observations'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent_dataset, test_agent_dataset = split_dict(agent_dataset, split_ratio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_agent_dataset['observations'].shape, test_agent_dataset['observations'].shape, synthetic_dataset['observations'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsModel(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(obs_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.mean_head = nn.Linear(hidden_dim, obs_dim)\n",
    "        self.log_std_head = nn.Linear(hidden_dim, obs_dim)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        h = self.fc(x)\n",
    "        mean = self.mean_head(h)\n",
    "        log_std = self.log_std_head(h)\n",
    "        log_std = torch.clamp(log_std, min=-5, max=1)  # Prevent extreme values\n",
    "        std = torch.exp(log_std)  # Ensure positivity\n",
    "        return mean, std\n",
    "\n",
    "def load_data(offline_data, batch_size=512, val_ratio=0.25):\n",
    "    states = offline_data['observations']\n",
    "    actions = offline_data['actions']\n",
    "    next_states = offline_data['next_observations']\n",
    "    \n",
    "    state_scaler = StandardScaler()\n",
    "    action_scaler = StandardScaler()\n",
    "    next_state_scaler = StandardScaler()\n",
    "    \n",
    "    states = state_scaler.fit_transform(states)\n",
    "    actions = action_scaler.fit_transform(actions)\n",
    "    next_states = next_state_scaler.fit_transform(next_states)\n",
    "    \n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.float32)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    \n",
    "    dataset = TensorDataset(states, actions, next_states)\n",
    "    val_size = int(len(dataset) * val_ratio)\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print('Train:', len(train_dataset), 'Val.:', len(val_dataset))\n",
    "    \n",
    "    return train_loader, val_loader, (state_scaler, action_scaler, next_state_scaler)\n",
    "\n",
    "def train_dynamics_model(offline_data, obs_dim, action_dim, epochs=20, lr=3e-4):\n",
    "\n",
    "    model = DynamicsModel(obs_dim, action_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_loader, val_loader, scalers = load_data(offline_data)\n",
    "    train_losses, val_losses = [] , []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "        for state, action, next_state in train_loader:\n",
    "            mean, std = model(state, action)\n",
    "            loss = ((next_state - mean) / std).pow(2).mean() + std.mean()  # Negative log-likelihood loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for state, action, next_state in val_loader:\n",
    "                mean, std = model(state, action)\n",
    "                val_loss = ((next_state - mean) / std).pow(2).mean() + std.mean()\n",
    "                total_val_loss += val_loss.item()\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    return model, train_losses, val_losses, scalers\n",
    "\n",
    "def evaluate_data(model, data, scalers):\n",
    "    \n",
    "    state_scaler, action_scaler, next_state_scaler = scalers\n",
    "    \n",
    "    states = state_scaler.transform(data['observations'])\n",
    "    actions = action_scaler.transform(data['actions'])\n",
    "    next_states = next_state_scaler.transform(data['next_observations'])\n",
    "    \n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.float32)\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        mean, std = model(states, actions)\n",
    "        log_likelihood = -(((next_states - mean) / std).pow(2) + torch.log(std)).mean(dim=1)\n",
    "    \n",
    "    return log_likelihood.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = composuite.make(robot, obj, obst, subtask, use_task_id_obs=False, ignore_done=False)\n",
    "model, train_losses, val_losses, scalers = train_dynamics_model(train_agent_dataset, env.obs_dim, env.action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_likelihoods = evaluate_data(model, agent_dataset, scalers)\n",
    "agent_likelihoods = np.exp(agent_likelihoods)\n",
    "\n",
    "plt.hist(agent_likelihoods, bins=30)\n",
    "plt.xlabel('Likelihood')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Likelihoods of Agent Transitions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_agent_likelihoods = evaluate_data(model, train_agent_dataset, scalers)\n",
    "train_agent_likelihoods = np.exp(train_agent_likelihoods)\n",
    "\n",
    "plt.hist(train_agent_likelihoods, bins=30)\n",
    "plt.xlabel('Likelihood')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Train: Likelihoods of Agent Transitions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_agent_likelihoods = evaluate_data(model, test_agent_dataset, scalers)\n",
    "test_agent_likelihoods = np.exp(test_agent_likelihoods)\n",
    "\n",
    "plt.hist(train_agent_likelihoods, bins=30)\n",
    "plt.xlabel('Likelihood')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Test: Likelihoods of Agent Transitions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_likelihoods = evaluate_data(model, synthetic_dataset, scalers)\n",
    "synthetic_likelihoods = np.exp(synthetic_likelihoods)\n",
    "\n",
    "plt.hist(synthetic_likelihoods, bins=30)\n",
    "plt.xlabel('Likelihood')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Likelihoods of Synthetic Transitions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_agent_likelihoods.mean().item(), test_agent_likelihoods.mean().item(), synthetic_likelihoods.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(train_agent_likelihoods, bins=30, alpha=0.5, label=\"Train: Agent Transitions\")\n",
    "# plt.hist(test_agent_likelihoods, bins=30, alpha=0.5, label=\"Test: Agent Transitions\")\n",
    "plt.hist(agent_likelihoods, bins=30, alpha=0.5, label=\"Agent Transitions\")\n",
    "plt.hist(synthetic_likelihoods, bins=30, alpha=0.5, label=\"Synthetic Transitions\")\n",
    "\n",
    "plt.xlabel(\"Likelihood\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Likelihoods of Agent and Synthetic Transitions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_likelihood_transitions(synthetic_data, likelihoods, top_percent=1):\n",
    "\n",
    "    num_top = int(len(likelihoods) * (top_percent/100))\n",
    "    top_indices = np.argsort(likelihoods)[-num_top:]\n",
    "    filtered_data = {key: val[top_indices] for key, val in synthetic_data.items()}\n",
    "\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_synthetic_dataset = filter_top_likelihood_transitions(synthetic_dataset, synthetic_likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'filtered_transitions.pkl'\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "    pickle.dump(filtered_synthetic_dataset, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
