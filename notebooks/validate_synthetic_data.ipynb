{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import composuite\n",
    "from diffusion.utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = 'IIWA'\n",
    "obj = 'Dumbbell'\n",
    "obst = 'ObjectDoor'\n",
    "subtask = 'Trashcan'\n",
    "\n",
    "representative_indicators_env = composuite.make(robot, obj, obst, subtask, use_task_id_obs=True, ignore_done=False)\n",
    "modality_dims = representative_indicators_env.modality_dims\n",
    "\n",
    "\n",
    "base_agent_data_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/data'\n",
    "dataset = load_single_composuite_dataset(base_path=base_agent_data_path, \n",
    "                                            dataset_type='expert', \n",
    "                                            robot=robot, obj=obj, \n",
    "                                            obst=obst, task=subtask)\n",
    "agent_dataset = transitions_dataset(dataset)\n",
    "agent_dataset, _ = remove_indicator_vectors(modality_dims, agent_dataset)\n",
    "\n",
    "base_synthetic_data_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/cluster_results/diffusion/cond_diff_20/train/'\n",
    "synthetic_dataset = load_single_synthetic_dataset(base_path=base_synthetic_data_path, \n",
    "                                                  robot=robot, obj=obj, \n",
    "                                                  obst=obst, task=subtask)\n",
    "\n",
    "print(agent_dataset['observations'].shape, synthetic_dataset['observations'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self, data, eps=1e-8):\n",
    "        self.mean = np.mean(data, axis=0)\n",
    "        self.std = np.std(data, axis=0) + eps\n",
    "\n",
    "    def normalize(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def denormalize(self, normalized_data):\n",
    "        return normalized_data * self.std + self.mean\n",
    "\n",
    "\n",
    "def prepare_train_data(agent_dataset, split_ratio=0.9):\n",
    "\n",
    "    obs_normalizer = Normalizer(agent_dataset['observations'])\n",
    "    act_normalizer = Normalizer(agent_dataset['actions'])\n",
    "    \n",
    "    norm_obs = obs_normalizer.normalize(agent_dataset['observations'])\n",
    "    norm_acts = act_normalizer.normalize(agent_dataset['actions'])\n",
    "    norm_next_obs = obs_normalizer.normalize(agent_dataset['next_observations'])\n",
    "    \n",
    "    obs = torch.tensor(norm_obs, dtype=torch.float32)\n",
    "    acts = torch.tensor(norm_acts, dtype=torch.float32)\n",
    "    next_obs = torch.tensor(norm_next_obs, dtype=torch.float32)\n",
    "    \n",
    "    dataset = TensorDataset(obs, acts, next_obs)\n",
    "    train_size = int(split_ratio * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    return (DataLoader(train_data, batch_size=512, shuffle=True), \n",
    "            DataLoader(val_data, batch_size=512, shuffle=False),\n",
    "            obs_normalizer, act_normalizer)\n",
    "\n",
    "\n",
    "def prepare_test_data(synthetic_dataset, obs_normalizer, act_normalizer):\n",
    "\n",
    "    norm_obs = obs_normalizer.normalize(synthetic_dataset['observations'])\n",
    "    norm_acts = act_normalizer.normalize(synthetic_dataset['actions'])\n",
    "    norm_next_obs = obs_normalizer.normalize(synthetic_dataset['next_observations'])\n",
    "    \n",
    "    obs = torch.tensor(norm_obs, dtype=torch.float32)\n",
    "    acts = torch.tensor(norm_acts, dtype=torch.float32)\n",
    "    next_obs = torch.tensor(norm_next_obs, dtype=torch.float32)\n",
    "    \n",
    "    dataset = TensorDataset(obs, acts, next_obs)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "\n",
    "class ProbabilisticDynamicsModel(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, obs_normalizer, act_normalizer, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.obs_normalizer = obs_normalizer\n",
    "        self.act_normalizer = act_normalizer\n",
    "\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.mean_head = nn.Linear(256, obs_dim)\n",
    "        self.log_var_head = nn.Linear(256, obs_dim)\n",
    "        self.log_var_scale = nn.Parameter(torch.ones(obs_dim) * 1.0)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
    "            module.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        x = self.shared_net(x)\n",
    "        mean = self.mean_head(x)\n",
    "        log_var = self.log_var_scale * torch.tanh(self.log_var_head(x))\n",
    "        return mean, log_var\n",
    "    \n",
    "    def predict(self, obs, act):\n",
    "        \"\"\" Make predictions in the original (unnormalized) space. \"\"\"\n",
    "\n",
    "        norm_obs = torch.tensor(self.obs_normalizer.normalize(obs), dtype=torch.float32)\n",
    "        norm_acts = torch.tensor(self.act_normalizer.normalize(act), dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            norm_mean, norm_log_var = self(norm_obs, norm_acts)\n",
    "        \n",
    "        mean = self.obs_normalizer.denormalize(norm_mean.numpy())\n",
    "        log_var = norm_log_var.numpy() + 2 * np.log(self.obs_normalizer.std)\n",
    "        \n",
    "        return mean, log_var\n",
    "    \n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=15, lr=1e-3):\n",
    "\n",
    "    criterion = nn.GaussianNLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for obs, acts, next_obs in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            mean, log_var = model(obs, acts)\n",
    "            var = torch.exp(log_var)\n",
    "            loss = criterion(mean, next_obs, var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for obs, acts, next_obs in val_loader:\n",
    "                mean, log_var = model(obs, acts)\n",
    "                var = torch.exp(log_var)\n",
    "                loss = criterion(mean, next_obs, var)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "def compute_likelihoods(model, dataloader):\n",
    "\n",
    "    criterion = nn.GaussianNLLLoss()\n",
    "    likelihoods = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for obs, acts, next_obs in dataloader:\n",
    "            mean, log_var = model(obs, acts)\n",
    "            var = torch.exp(log_var)\n",
    "            nll = criterion(mean, next_obs, var).cpu().numpy()\n",
    "            likelihoods.append(-nll)\n",
    "\n",
    "    return np.array(likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = composuite.make(robot, obj, obst, subtask, use_task_id_obs=False, ignore_done=False)\n",
    "\n",
    "train_loader, val_loader, obs_normalizer, act_normalizer = prepare_train_data(agent_dataset)\n",
    "test_loader = prepare_test_data(synthetic_dataset, obs_normalizer, act_normalizer)\n",
    "\n",
    "model = ProbabilisticDynamicsModel(\n",
    "    obs_dim=env.obs_dim, \n",
    "    act_dim=env.action_dim,\n",
    "    obs_normalizer=obs_normalizer,\n",
    "    act_normalizer=act_normalizer\n",
    ")\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_likelihoods = compute_likelihoods(model, train_loader)\n",
    "val_likelihoods = compute_likelihoods(model, val_loader)\n",
    "test_likelihoods = compute_likelihoods(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(map(np.min, [train_likelihoods, val_likelihoods, test_likelihoods])), \n",
    "                   max(map(np.max, [train_likelihoods, val_likelihoods, test_likelihoods])), \n",
    "                   10)\n",
    "\n",
    "plt.hist(train_likelihoods, bins=bins, alpha=0.5, label='Train Set')\n",
    "plt.hist(val_likelihoods, bins=bins, alpha=0.5, label='Val. Set')\n",
    "plt.hist(test_likelihoods, bins=bins, alpha=0.5, label='Test Set')\n",
    "\n",
    "plt.xlabel('Log Likelihood')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dimensional_likelihoods(model, dataloader):\n",
    "    \"\"\"Compute per-dimension negative log likelihoods.\n",
    "    Returns:\n",
    "        all_dim_nlls: Array of shape (n_samples, n_dims) containing per-dimension NLL\n",
    "    \"\"\"\n",
    "    all_dim_nlls = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for obs, acts, next_obs in dataloader:\n",
    "            mean, log_var = model(obs, acts)\n",
    "            var = torch.exp(log_var)\n",
    "            # Compute per-dimension NLL\n",
    "            # NLL = 0.5 * (log(2π) + log(σ²) + (x-μ)²/σ²)\n",
    "            sq_mahalanobis = (next_obs - mean)**2 / var\n",
    "            log_det = log_var\n",
    "            dim_nll = 0.5 * (np.log(2 * np.pi) + log_det + sq_mahalanobis)\n",
    "            all_dim_nlls.append(dim_nll.cpu().numpy())\n",
    "    \n",
    "    all_dim_nlls = np.concatenate(all_dim_nlls, axis=0)\n",
    "    return all_dim_nlls\n",
    "\n",
    "def analyze_dimensional_differences(train_loader, test_loader, model):\n",
    "    \"\"\" Analyze which dimensions contribute most to likelihood differences. \"\"\"\n",
    "    \n",
    "    train_dim_nlls = compute_dimensional_likelihoods(model, train_loader)\n",
    "    test_dim_nlls = compute_dimensional_likelihoods(model, test_loader)\n",
    "    \n",
    "    train_means = np.mean(train_dim_nlls, axis=0)\n",
    "    test_means = np.mean(test_dim_nlls, axis=0)\n",
    "    differences = test_means - train_means\n",
    "    \n",
    "    worst_dims = np.argsort(differences)[::-1]\n",
    "    \n",
    "    summary = []\n",
    "    for dim in worst_dims:\n",
    "        summary.append({\n",
    "            'dimension': dim,\n",
    "            'train_nll': train_means[dim],\n",
    "            'test_nll': test_means[dim],\n",
    "            'difference': differences[dim]\n",
    "        })\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = analyze_dimensional_differences(train_loader, test_loader, model)\n",
    "\n",
    "print(\"\\nDimensions sorted by likelihood difference (worst to best):\")\n",
    "for entry in summary:\n",
    "    print(f\"Dimension {entry['dimension']:3d}: \"\n",
    "          f\"Train NLL = {entry['train_nll']:6.3f}, \"\n",
    "          f\"Test NLL = {entry['test_nll']:6.3f}, \"\n",
    "          f\"Diff = {entry['difference']:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dimensional_differences(train_loader, test_loader, model):\n",
    "    \"\"\"Visualize per-dimension likelihood differences between train and test data.\"\"\"\n",
    "    \n",
    "    train_dim_nlls = compute_dimensional_likelihoods(model, train_loader)\n",
    "    test_dim_nlls = compute_dimensional_likelihoods(model, test_loader)\n",
    "    \n",
    "    train_means = np.mean(train_dim_nlls, axis=0)\n",
    "    test_means = np.mean(test_dim_nlls, axis=0)\n",
    "    differences = test_means - train_means\n",
    "    \n",
    "    worst_dims_idx = np.argsort(differences)[::-1]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), height_ratios=[1.5, 1])\n",
    "    \n",
    "    n_dims = len(worst_dims_idx)\n",
    "    top_k = min(10, n_dims)  # Show top 10 worst dimensions\n",
    "    worst_dims = worst_dims_idx[:top_k]\n",
    "    \n",
    "    plot_data = []\n",
    "    labels = []\n",
    "    for dim in worst_dims:\n",
    "        plot_data.extend([train_dim_nlls[:, dim], test_dim_nlls[:, dim]])\n",
    "        labels.extend([f'Train Dim {dim}', f'Test Dim {dim}'])\n",
    "    \n",
    "    sns.boxplot(data=plot_data, ax=ax1)\n",
    "    ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax1.set_title('Distribution of Negative Log-Likelihoods by Dimension\\n(Top 10 Most Different Dimensions)')\n",
    "    ax1.set_ylabel('Negative Log-Likelihood')\n",
    "    \n",
    "    dims = np.arange(len(worst_dims))\n",
    "    diffs = differences[worst_dims]\n",
    "    \n",
    "    bars = ax2.bar(dims, diffs)\n",
    "    \n",
    "    colors = plt.cm.RdYlBu_r(np.linspace(0, 1, len(bars)))\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    ax2.set_xticks(dims)\n",
    "    ax2.set_xticklabels([f'Dim {d}' for d in worst_dims], rotation=45, ha='right')\n",
    "    ax2.set_title('Difference in Mean NLL (Test - Train)\\nLarger Values = Worse Synthetic Data')\n",
    "    ax2.set_ylabel('Difference in NLL')\n",
    "    \n",
    "    textstr = '\\n'.join([\n",
    "        'Summary Statistics:',\n",
    "        f'Max Difference: {np.max(differences):.3f}',\n",
    "        f'Mean Difference: {np.mean(differences):.3f}',\n",
    "        f'Median Difference: {np.median(differences):.3f}'\n",
    "    ])\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax2.text(0.02, 0.98, textstr, transform=ax2.transAxes, fontsize=9,\n",
    "             verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return worst_dims, differences[worst_dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_dims, diffs = visualize_dimensional_differences(train_loader, test_loader, model)\n",
    "\n",
    "print(\"\\nTop differences by dimension:\")\n",
    "for dim, diff in zip(worst_dims, diffs):\n",
    "    print(f\"Dimension {dim:3d}: {diff:6.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
