{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from offline_compositional_rl_datasets.utils.data_utils import *\n",
    "\n",
    "task_list_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/offline_compositional_rl_datasets/_train_test_splits'\n",
    "base_logs_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/scripts/policies_slurm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_file(base_logs_path, \n",
    "                 dataset_type, \n",
    "                 algorithm, algorithm_seed, \n",
    "                 task_list_seed, \n",
    "                 robot, obj, obst, subtask, \n",
    "                 model_run=None, denoiser=None, num_train=None):\n",
    "    \"\"\"\n",
    "    Find the appropriate log file based on supplied parameters\n",
    "    \n",
    "    Args:\n",
    "        dataset_type: 'synthetic' or 'expert'\n",
    "        algorithm: algorithm name (e.g., 'td3_bc')\n",
    "        algorithm_seed: algorithm seed value\n",
    "        task_list_seed: task list seed value\n",
    "        robot: robot type (e.g., 'Jaco')\n",
    "        obj: object type (e.g., 'Hollowbox')\n",
    "        obst: obstacle type or 'None'\n",
    "        subtask: subtask name (e.g., 'Trashcan')\n",
    "        model_run: model run number (required for 'synthetic')\n",
    "        denoiser: denoiser type (required for 'synthetic')\n",
    "        num_train: number of training tasks in diffusion model\n",
    "    \n",
    "    Returns:\n",
    "        Path to the matching log file or None if not found\n",
    "    \"\"\"\n",
    "    \n",
    "    if dataset_type == \"synthetic\":\n",
    "        if model_run is None or denoiser is None:\n",
    "            raise ValueError(\"model_run and denoiser required for synthetic dataset\")\n",
    "            \n",
    "        if algorithm == 'td3_bc':\n",
    "            algorithm1 = 'td3'\n",
    "            algorithm2 = 'bc'\n",
    "            pattern = f\"*_{algorithm1}_{algorithm2}_{algorithm_seed}_{denoiser}_{task_list_seed}_{num_train}_{model_run}_\" \\\n",
    "                    f\"{robot}_{obj}_{obst}_{subtask}.out\"\n",
    "        else:\n",
    "            pattern = f\"*_{algorithm}_{algorithm_seed}_{denoiser}_{task_list_seed}_{num_train}_{model_run}_\" \\\n",
    "                    f\"{robot}_{obj}_{obst}_{subtask}.out\" \n",
    "    \n",
    "    elif dataset_type == \"expert\":\n",
    "        pattern = f\"*_{algorithm}_{algorithm_seed}_tl_{task_list_seed}_\" \\\n",
    "                 f\"{robot}_{obj}_{obst}_{subtask}.out\"\n",
    "    else:\n",
    "        raise ValueError(\"dataset_type must be 'synthetic' or 'expert'\")\n",
    "    \n",
    "    search_path = os.path.join(base_logs_path, pattern)\n",
    "    matching_files = glob.glob(search_path)\n",
    "    \n",
    "    return matching_files[0] if matching_files else None\n",
    "\n",
    "\n",
    "def parse_log_file(file_path):\n",
    "    \"\"\"Extract the final best score from a log file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "        match = re.search(r'Training completed\\. Best score: ([0-9]+\\.[0-9]+)', content)\n",
    "        if match:\n",
    "            return float(match.group(1))\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing log file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def run_evaluation_sweep(base_logs_path, task_list_path, task_list_seeds, \n",
    "                         algorithm, algorithm_seeds,\n",
    "                         dataset_type, \n",
    "                         experiment_type='default', holdout_elem=None,\n",
    "                         denoiser=None, num_train=None, model_run=1):\n",
    "    \"\"\"\n",
    "    Run evaluation sweep across multiple seeds and tasks.\n",
    "    For synthetic data, denoiser and model_run must be provided.\n",
    "    \"\"\"\n",
    "    results = defaultdict(list)\n",
    "    # Loop over task list seeds\n",
    "    for task_list_seed in task_list_seeds:\n",
    "        # Get task list for this seed\n",
    "        _, _, _, test_task_list = get_task_list(\n",
    "            task_list_path,\n",
    "            dataset_type,\n",
    "            experiment_type,\n",
    "            holdout_elem,  \n",
    "            task_list_seed,\n",
    "        )        \n",
    "        # Loop over algorithm seeds\n",
    "        for algorithm_seed in algorithm_seeds:\n",
    "            # Loop over each task in the task list\n",
    "            for task_num, (robot, obj, obst, subtask) in enumerate(test_task_list):\n",
    "                task_id = f\"{robot}_{obj}_{obst}_{subtask}\"\n",
    "                # Get log file path\n",
    "                log_file = get_log_file(\n",
    "                    base_logs_path, \n",
    "                    dataset_type, \n",
    "                    algorithm, \n",
    "                    algorithm_seed, \n",
    "                    task_list_seed, \n",
    "                    robot, obj, obst, subtask,\n",
    "                    model_run=model_run,\n",
    "                    denoiser=denoiser,\n",
    "                    num_train=num_train\n",
    "                )\n",
    "                if log_file:\n",
    "                    # Parse the log file to get the best score\n",
    "                    score = parse_log_file(log_file)\n",
    "                    if score is not None:\n",
    "                        # Store result\n",
    "                        results[task_id].append((task_list_seed, algorithm_seed, score))\n",
    "                    else:\n",
    "                        print(f\"Task {task_num + 1}: {task_id} - Failed to parse score\")\n",
    "                else:\n",
    "                    print(f\"Task {task_num + 1}: {task_id} - No log file found\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_aggregate_results_dataframe(results):\n",
    "\n",
    "    df_results = []\n",
    "    for task_id, scores in results.items():\n",
    "        # Group scores by task_list_seed\n",
    "        seed_scores = defaultdict(list)\n",
    "        for task_list_seed, algorithm_seed, score in scores:\n",
    "            seed_scores[task_list_seed].append(score)\n",
    "        \n",
    "        # Calculate average and standard error for each task_list_seed\n",
    "        for task_list_seed, seed_data in seed_scores.items():\n",
    "            avg_score = np.mean(seed_data)\n",
    "            std_error = np.std(seed_data)/np.sqrt(len(seed_data))\n",
    "            \n",
    "            df_results.append({\n",
    "                'task_id': task_id,\n",
    "                'task_list_seed': task_list_seed,\n",
    "                'avg_score': avg_score,\n",
    "                'std_error': std_error,\n",
    "                'num_rl_seeds': len(seed_data)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(df_results)\n",
    "\n",
    "\n",
    "def create_results_dataframe(results):\n",
    "    df_results = []\n",
    "    \n",
    "    for task_id, scores in results.items():\n",
    "        # Extract the robot, object, obstacle, subtask components\n",
    "        robot, obj, obst, subtask = task_id.split('_')\n",
    "        \n",
    "        # Add each individual data point to preserve all seed information\n",
    "        for task_list_seed, algorithm_seed, score in scores:\n",
    "            df_results.append({\n",
    "                'task_id': task_id,\n",
    "                'robot': robot,\n",
    "                'object': obj,\n",
    "                'obstacle': obst,\n",
    "                'subtask': subtask,\n",
    "                'task_list_seed': task_list_seed,\n",
    "                'algorithm_seed': algorithm_seed,\n",
    "                'score': score\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(df_results)\n",
    "\n",
    "\n",
    "def percent_diff(synthetic, expert):\n",
    "    return ((synthetic - expert) / expert) * 100\n",
    "\n",
    "\n",
    "def create_comparison_pivot(combined_df, task_list_seed, num_train):\n",
    "   \"\"\"\n",
    "   Create a pivot table comparing expert and synthetic scores for a specific task_list_seed and num_train\n",
    "   \n",
    "   Args:\n",
    "       combined_df: The combined dataframe with all results\n",
    "       task_list_seed: Task list seed to filter by\n",
    "       num_train: Number of training examples to include from synthetic data\n",
    "       \n",
    "   Returns:\n",
    "       Pivot table with tasks as rows and scores as columns, sorted by expert score\n",
    "   \"\"\"\n",
    "\n",
    "   filtered_df = combined_df[combined_df['task_list_seed'] == task_list_seed]\n",
    "   filtered_df = filtered_df[\n",
    "       (filtered_df['data_source'] == 'expert') | \n",
    "       (filtered_df['num_train'] == num_train)\n",
    "   ]\n",
    "   \n",
    "   filtered_df['source_seed'] = filtered_df.apply(\n",
    "       lambda row: 'expert_seed_' + str(row['algorithm_seed']) if row['data_source'] == 'expert' \n",
    "                   else 'synth_' + str(row['num_train']) + '_seed_' + str(row['algorithm_seed']),\n",
    "       axis=1\n",
    "   )\n",
    "   \n",
    "   pivot_df = filtered_df.pivot(\n",
    "       index='task_id',\n",
    "       columns='source_seed',\n",
    "       values='score'\n",
    "   )\n",
    "   \n",
    "   if any(col.startswith('expert') for col in pivot_df.columns):\n",
    "       expert_col = [col for col in pivot_df.columns if col.startswith('expert')][0]\n",
    "       pivot_df = pivot_df.sort_values(by=expert_col, ascending=False)\n",
    "   \n",
    "   return pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = 'td3_bc'\n",
    "task_list_seeds = [0, 1]\n",
    "algorithm_seeds = [0, 1]\n",
    "num_train_tasks = [56, 98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_results = run_evaluation_sweep(\n",
    "    base_logs_path=base_logs_path,\n",
    "    task_list_path=task_list_path,\n",
    "    task_list_seeds=task_list_seeds,\n",
    "    algorithm=algorithm,\n",
    "    algorithm_seeds=algorithm_seeds,\n",
    "    dataset_type=\"expert\",\n",
    ")\n",
    "expert_df = create_aggregate_results_dataframe(expert_results)\n",
    "expert_df['data_source'] = 'expert'\n",
    "expert_df['num_train'] = None\n",
    "expert_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = expert_df.groupby('task_list_seed').agg({\n",
    "    'avg_score': 'mean',\n",
    "    'std_error': lambda x: np.sqrt(np.sum(x**2)) / len(x)  # Propagate error\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Expert data:\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_results_dfs = []\n",
    "\n",
    "for num_train in num_train_tasks:\n",
    "    synthetic_results = run_evaluation_sweep(\n",
    "        base_logs_path=base_logs_path,\n",
    "        task_list_path=task_list_path,\n",
    "        task_list_seeds=task_list_seeds,\n",
    "        algorithm=algorithm,\n",
    "        algorithm_seeds=algorithm_seeds,\n",
    "        dataset_type=\"synthetic\",\n",
    "        denoiser='monolithic',\n",
    "        num_train=num_train\n",
    "    )\n",
    "    synthetic_results_df = create_aggregate_results_dataframe(synthetic_results)\n",
    "    synthetic_results_dfs.append(synthetic_results_df)\n",
    "\n",
    "    summary = synthetic_results_df.groupby('task_list_seed').agg({\n",
    "        'avg_score': 'mean',\n",
    "        'std_error': lambda x: np.sqrt(np.sum(x**2)) / len(x)  # Propagate error\n",
    "    }).reset_index()\n",
    "\n",
    "    print(f\"Synthetic data; Training tasks: {num_train}:\")\n",
    "    display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = expert_df.copy()\n",
    "\n",
    "for i, num_train in enumerate(num_train_tasks):\n",
    "    synthetic_df = synthetic_results_dfs[i].copy()\n",
    "    synthetic_df['data_source'] = 'synthetic'\n",
    "    synthetic_df['num_train'] = num_train\n",
    "    combined_df = pd.concat([combined_df, synthetic_df], ignore_index=True)\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group expert data by task_id and task_list_seed to get baseline scores\n",
    "expert_scores = combined_df[combined_df['data_source'] == 'expert'].groupby(['task_id', 'task_list_seed'])['avg_score'].mean().reset_index()\n",
    "\n",
    "# Group synthetic data by task_id, task_list_seed, and num_train\n",
    "synthetic_scores = combined_df[combined_df['data_source'] == 'synthetic'].groupby(['task_id', 'task_list_seed', 'num_train'])['avg_score'].mean().reset_index()\n",
    "\n",
    "percent_diffs = []\n",
    "# For each task_id and task_list_seed, calculate percentage difference\n",
    "for (task, seed) in expert_scores[['task_id', 'task_list_seed']].values:\n",
    "    # Get the expert score for this task and seed\n",
    "    expert_score = expert_scores[(expert_scores['task_id'] == task) & \n",
    "                               (expert_scores['task_list_seed'] == seed)]['avg_score'].values[0]\n",
    "    # Get all synthetic scores for this task and seed\n",
    "    task_synth_scores = synthetic_scores[(synthetic_scores['task_id'] == task) & \n",
    "                                       (synthetic_scores['task_list_seed'] == seed)]\n",
    "    for _, row in task_synth_scores.iterrows():\n",
    "        percent_diffs.append({\n",
    "            'task_id': task,\n",
    "            'task_list_seed': seed,\n",
    "            'num_train': row['num_train'],\n",
    "            'expert_score': expert_score,\n",
    "            'synthetic_score': row['avg_score'],\n",
    "            'percent_difference': percent_diff(row['avg_score'], expert_score)\n",
    "        })\n",
    "diff_df = pd.DataFrame(percent_diffs)\n",
    "# Compute average percentage difference for each num_train across all tasks/seeds\n",
    "avg_diffs = diff_df.groupby('num_train')['percent_difference'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "print(\"Average percentage difference by number of training tasks:\")\n",
    "print(avg_diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(avg_diffs['num_train'], avg_diffs['mean'], yerr=avg_diffs['std'], \n",
    "             marker='o', linestyle='-', capsize=5, linewidth=2, label='Monolithic Model Data')\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.7, label='Expert')\n",
    "plt.xlabel('Number of Training Tasks', fontsize=12)\n",
    "plt.ylabel('Percentage Difference from Expert Score (%)', fontsize=12)\n",
    "plt.title('Score Gap: Synthetic and Expert Data', fontsize=12)\n",
    "plt.grid(True, alpha=0.5)\n",
    "\n",
    "plt.xticks(avg_diffs['num_train'], fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Add annotations for each point\n",
    "for i, row in avg_diffs.iterrows():\n",
    "    plt.annotate(f\"{row['mean']:.1f}%\", \n",
    "                (row['num_train'], row['mean']),\n",
    "                textcoords=\"offset points\",\n",
    "                xytext=(0,10),\n",
    "                ha='center',\n",
    "                fontsize=12)\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_expert_df = create_results_dataframe(expert_results)\n",
    "fine_synthetic_results_dfs = []\n",
    "\n",
    "for num_train in num_train_tasks:\n",
    "    synthetic_results = run_evaluation_sweep(\n",
    "        base_logs_path=base_logs_path,\n",
    "        task_list_path=task_list_path,\n",
    "        task_list_seeds=task_list_seeds,\n",
    "        algorithm=algorithm,\n",
    "        algorithm_seeds=algorithm_seeds,\n",
    "        dataset_type=\"synthetic\",\n",
    "        denoiser='monolithic',\n",
    "        num_train=num_train\n",
    "    )\n",
    "    fine_synthetic_results_df = create_results_dataframe(synthetic_results)\n",
    "    fine_synthetic_results_dfs.append(fine_synthetic_results_df)\n",
    "\n",
    "fine_combined_df = fine_expert_df.copy()\n",
    "fine_combined_df['data_source'] = 'expert'\n",
    "fine_combined_df['num_train'] = None  # Not applicable for expert data\n",
    "\n",
    "# Add each synthetic dataset with appropriate markers\n",
    "for i, num_train in enumerate(num_train_tasks):\n",
    "    synthetic_df = fine_synthetic_results_dfs[i].copy()\n",
    "    synthetic_df['data_source'] = 'synthetic'\n",
    "    synthetic_df['num_train'] = num_train\n",
    "    fine_combined_df = pd.concat([fine_combined_df, synthetic_df], ignore_index=True)\n",
    "\n",
    "fine_combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list_seed_of_interest = 0\n",
    "num_train_of_interest = 56 \n",
    "comparison_df = create_comparison_pivot(fine_combined_df, task_list_seed_of_interest, num_train_of_interest)\n",
    "comparison_df.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list_seed_of_interest = 0\n",
    "num_train_of_interest = 98\n",
    "comparison_df = create_comparison_pivot(fine_combined_df, task_list_seed_of_interest, num_train_of_interest)\n",
    "comparison_df.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list_seed_of_interest = 1\n",
    "num_train_of_interest = 56 \n",
    "comparison_df = create_comparison_pivot(fine_combined_df, task_list_seed_of_interest, num_train_of_interest)\n",
    "comparison_df.head(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list_seed_of_interest = 1\n",
    "num_train_of_interest = 98\n",
    "comparison_df = create_comparison_pivot(fine_combined_df, task_list_seed_of_interest, num_train_of_interest)\n",
    "comparison_df.head(32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
