{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import composuite\n",
    "from diffusion.utils import *\n",
    "from online.online_exp import *\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_mean_std(states: np.ndarray, eps: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    mean = states.mean(0)\n",
    "    std = states.std(0) + eps\n",
    "    return mean, std\n",
    "\n",
    "def wrap_env(\n",
    "    env: gym.Env,\n",
    "    state_mean: Union[np.ndarray, float] = 0.0,\n",
    "    state_std: Union[np.ndarray, float] = 1.0,\n",
    "    reward_scale: float = 1.0,\n",
    ") -> gym.Env:\n",
    "    # PEP 8: E731 do not assign a lambda expression, use a def\n",
    "    def normalize_state(state):\n",
    "        return (\n",
    "            state - state_mean\n",
    "        ) / state_std  # epsilon should be already added in std.\n",
    "\n",
    "    def scale_reward(reward):\n",
    "        # Please be careful, here reward is multiplied by scale!\n",
    "        return reward_scale * reward\n",
    "\n",
    "    env = gym.wrappers.TransformObservation(env, normalize_state)\n",
    "    if reward_scale != 1.0:\n",
    "        env = gym.wrappers.TransformReward(env, scale_reward)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gin.parse_config_file(\"/Users/shubhankar/Developer/compositional-rl-synth-data/config/sac.gin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_agent_data_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/data'\n",
    "base_synthetic_data_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/cluster_results/diffusion'\n",
    "\n",
    "base_results_folder = '/Users/shubhankar/Developer/compositional-rl-synth-data/local_results/offline_learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = 'Jaco'\n",
    "obj = 'Plate'\n",
    "obst = 'GoalWall'\n",
    "subtask = 'PickPlace'\n",
    "\n",
    "env_name = f\"{robot}_{obj}_{obst}_{subtask}\"\n",
    "\n",
    "data_type = 'agent'\n",
    "\n",
    "if data_type == 'synthetic':\n",
    "    synthetic_run_id = 20\n",
    "    mode = 'train'\n",
    "\n",
    "if data_type == 'agent':\n",
    "    env = composuite.make(robot, obj, obst, subtask, use_task_id_obs=True, ignore_done=False)\n",
    "    dataset = load_single_composuite_dataset(base_path=base_agent_data_path, \n",
    "                                             dataset_type='expert', \n",
    "                                             robot=robot, obj=obj, \n",
    "                                             obst=obst, task=subtask)\n",
    "    dataset, _ = remove_indicator_vectors(env.modality_dims, transitions_dataset(dataset))\n",
    "\n",
    "if data_type == 'synthetic':\n",
    "    dataset = load_single_synthetic_dataset(base_path=os.path.join(base_synthetic_data_path, synthetic_run_id, mode), \n",
    "                                            robot=robot, obj=obj, \n",
    "                                            obst=obst, task=subtask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_full = f\"redq_sac_{robot}_{obj}_{obst}_{subtask}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results_path = Path(base_results_folder)\n",
    "idx = 1\n",
    "while (base_results_path / f\"offline_learning_redq_sac_{data_type}_{idx}\").exists():\n",
    "    idx += 1\n",
    "results_folder = base_results_path / f\"offline_learning_redq_sac_{data_type}_{idx}\"\n",
    "results_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger_kwargs = setup_logger_kwargs(exp_name_full, data_dir=results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "epochs = 100\n",
    "steps_per_epoch = 1000\n",
    "max_ep_len = 1000\n",
    "n_evals_per_epoch = 1\n",
    "\n",
    "# Agent hyperparameters.\n",
    "hidden_sizes = (256, 256)\n",
    "replay_size = int(1e6)\n",
    "batch_size = 1024\n",
    "lr = 3e-4\n",
    "gamma = 0.99\n",
    "polyak = 0.995\n",
    "alpha = 0.2\n",
    "auto_alpha = True\n",
    "target_entropy = 'auto'\n",
    "start_steps = 5000\n",
    "delay_update_steps = 'auto'\n",
    "utd_ratio = 20\n",
    "num_Q = 10\n",
    "num_min = 2\n",
    "q_target_mode = 'min'\n",
    "policy_update_delay = 20\n",
    "diffusion_buffer_size = int(1e6)\n",
    "diffusion_sample_ratio = 0.5\n",
    "\n",
    "# Diffusion hyperparameters.\n",
    "retrain_diffusion_every = 10_000\n",
    "num_samples = 100_000\n",
    "diffusion_start = 0\n",
    "disable_diffusion = True\n",
    "print_buffer_stats = True\n",
    "skip_reward_norm = True\n",
    "model_terminals = False\n",
    "\n",
    "# Bias evaluation.\n",
    "evaluate_bias = True\n",
    "n_mc_eval = 1000\n",
    "n_mc_cutoff = 350\n",
    "reseed_each_epoch = True\n",
    "\n",
    "# W&B\n",
    "project_name = 'diffusion_online'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = steps_per_epoch * epochs + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Set up logger. \"\"\"\n",
    "logger = EpochLogger(**logger_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Set up environment/s. \"\"\"\n",
    "state_mean, state_std = compute_mean_std(dataset[\"observations\"], eps=1e-3)\n",
    "env_fn = lambda: wrap_env(composuite.make(robot, obj, obst, subtask), state_mean=state_mean, state_std=state_std)\n",
    "env, test_env, bias_eval_env = env_fn(), env_fn(), env_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Set up seeding. \"\"\"\n",
    "# Seed torch and numpy.\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# seed environment along with env action space so that everything is properly seeded for reproducibility\n",
    "def seed_all(epoch):\n",
    "    seed_shift = epoch * 9999\n",
    "    mod_value = 999999\n",
    "    env_seed = (seed + seed_shift) % mod_value\n",
    "    test_env_seed = (seed + 10000 + seed_shift) % mod_value\n",
    "    bias_eval_env_seed = (seed + 20000 + seed_shift) % mod_value\n",
    "    torch.manual_seed(env_seed)\n",
    "    np.random.seed(env_seed)\n",
    "    env.seed(env_seed)\n",
    "    env.action_space.np_random.seed(env_seed)\n",
    "    test_env.seed(test_env_seed)\n",
    "    test_env.action_space.np_random.seed(test_env_seed)\n",
    "    bias_eval_env.seed(bias_eval_env_seed)\n",
    "    bias_eval_env.action_space.np_random.seed(bias_eval_env_seed)\n",
    "\n",
    "seed_all(epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Prepare to initialize agent. \"\"\"\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "max_ep_len = 1000\n",
    "# Action limit for clamping; assumes all dimensions share the same bound!\n",
    "# Need .item() to convert it from NumPy float to Python float.\n",
    "act_limit = env.action_space.high[0].item()\n",
    "start_time = time.time()\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialize agent + buffer and begin training. \"\"\"\n",
    "agent_config = {\n",
    "    'env_name': env_name,\n",
    "    'hidden_sizes': hidden_sizes,\n",
    "    'replay_size': replay_size,\n",
    "    'batch_size': batch_size,\n",
    "    'lr': lr,\n",
    "    'gamma': gamma,\n",
    "    'polyak': polyak,\n",
    "    'alpha': alpha,\n",
    "    'auto_alpha': auto_alpha,\n",
    "    'target_entropy': target_entropy,\n",
    "    'start_steps': start_steps,\n",
    "    'delay_update_steps': delay_update_steps,\n",
    "    'utd_ratio': utd_ratio,\n",
    "    'num_Q': num_Q,\n",
    "    'num_min': num_min,\n",
    "    'q_target_mode': q_target_mode,\n",
    "    'policy_update_delay': policy_update_delay,\n",
    "}\n",
    "\n",
    "wandb.init(project=project_name, name=logger_kwargs['exp_name'])\n",
    "wandb.config.update(agent_config)\n",
    "\n",
    "agent = REDQRLPDAgent(diffusion_buffer_size, diffusion_sample_ratio, \n",
    "                      env_name, obs_dim, act_dim, act_limit, device,\n",
    "                      hidden_sizes, replay_size, batch_size,\n",
    "                      lr, gamma, polyak,\n",
    "                      alpha, auto_alpha, target_entropy,\n",
    "                      start_steps, delay_update_steps,\n",
    "                      utd_ratio, num_Q, num_min, q_target_mode,\n",
    "                      policy_update_delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.replay_buffer.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Interaction Buffer Fraction: {agent.replay_buffer.size/agent.replay_buffer.max_size}')\n",
    "print(f'Diffusion Buffer Fraction: {agent.diffusion_buffer.size/agent.diffusion_buffer.max_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = dataset['observations']\n",
    "actions = dataset['actions']\n",
    "rewards = dataset['rewards']\n",
    "next_observations = dataset['next_observations']\n",
    "terminals = dataset['terminals']\n",
    "\n",
    "for o, a, r, o2, term in zip(observations, actions, rewards, next_observations, terminals):\n",
    "    agent.diffusion_buffer.store(o, a, r, o2, term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up diffusion model.\n",
    "diff_dims = obs_dim + act_dim + 1 + obs_dim\n",
    "if model_terminals:\n",
    "    diff_dims += 1\n",
    "inputs = torch.zeros((128, diff_dims)).float()\n",
    "if skip_reward_norm:\n",
    "    skip_dims = [obs_dim + act_dim]\n",
    "else:\n",
    "    skip_dims = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "\n",
    "for t in range(total_steps):\n",
    "    \n",
    "    a = agent.get_exploration_action(o, env)  # get action from agent\n",
    "    o2, r, d, _ = env.step(a)  # step the env, get next observation, reward and done signal\n",
    "\n",
    "    \"\"\" \n",
    "    Very important: Before we let agent store this transition,\n",
    "    ignore the \"done\" signal if it comes from hitting the time\n",
    "    horizon (that is, when it's an artificial terminal signal\n",
    "    that isn't based on the agent's state).\n",
    "    \"\"\"\n",
    "    ep_len += 1\n",
    "    d = False if ep_len == max_ep_len else d\n",
    "\n",
    "    agent.store_data(o, a, r, o2, d)  # give new data to replay buffer\n",
    "    agent.train(logger)  # let agent update\n",
    "    o = o2  # set obs to next obs\n",
    "    ep_ret += r\n",
    "\n",
    "    if d or (ep_len == max_ep_len):\n",
    "        logger.store(EpRet=ep_ret, EpLen=ep_len)  # store episode return and length to logger\n",
    "        o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0  # reset environment\n",
    "\n",
    "    if not disable_diffusion and (t + 1) % retrain_diffusion_every == 0 and (t + 1) >= diffusion_start:\n",
    "        print(f'Retraining diffusion model at step {t + 1}.')\n",
    "\n",
    "        # Train new diffusion model.\n",
    "        diffusion_trainer = REDQTrainer(\n",
    "            construct_diffusion_model(\n",
    "                inputs=inputs,\n",
    "                skip_dims=skip_dims,\n",
    "                disable_terminal_norm=model_terminals,\n",
    "            ),\n",
    "            results_folder=args.results_folder,\n",
    "            model_terminals=model_terminals,\n",
    "        )\n",
    "        diffusion_trainer.update_normalizer(agent.replay_buffer, device=device)\n",
    "        diffusion_trainer.train_from_redq_buffer(agent.replay_buffer)\n",
    "        agent.reset_diffusion_buffer()\n",
    "\n",
    "        # Add samples to replay buffer.\n",
    "        generator = SimpleDiffusionGenerator(env=env, ema_model=diffusion_trainer.ema.ema_model)\n",
    "        observations, actions, rewards, next_observations, terminals = generator.sample(num_samples=num_samples)\n",
    "\n",
    "        print(f'Adding {num_samples} samples to replay buffer.')\n",
    "        for o, a, r, o2, term in zip(observations, actions, rewards, next_observations, terminals):\n",
    "            agent.diffusion_buffer.store(o, a, r, o2, term)\n",
    "\n",
    "        if print_buffer_stats:\n",
    "            ptr_location = agent.replay_buffer.ptr\n",
    "            real_observations = agent.replay_buffer.obs1_buf[:ptr_location]\n",
    "            real_actions = agent.replay_buffer.acts_buf[:ptr_location]\n",
    "            real_next_observations = agent.replay_buffer.obs2_buf[:ptr_location]\n",
    "            real_rewards = agent.replay_buffer.rews_buf[:ptr_location]\n",
    "            print('Buffer stats:')\n",
    "            for i in range(observations.shape[1]):\n",
    "                print(f'Diffusion Obs {i}: {np.mean(observations[:, i]):.2f} {np.std(observations[:, i]):.2f}')\n",
    "                print(\n",
    "                    f'     Real Obs {i}: {np.mean(real_observations[:, i]):.2f} {np.std(real_observations[:, i]):.2f}')\n",
    "            for i in range(actions.shape[1]):\n",
    "                print(f'Diffusion Action {i}: {np.mean(actions[:, i]):.2f} {np.std(actions[:, i]):.2f}')\n",
    "                print(f'     Real Action {i}: {np.mean(real_actions[:, i]):.2f} {np.std(real_actions[:, i]):.2f}')\n",
    "            print(f'Diffusion Reward: {np.mean(rewards):.2f} {np.std(rewards):.2f}')\n",
    "            print(f'     Real Reward: {np.mean(real_rewards):.2f} {np.std(real_rewards):.2f}')\n",
    "            print(f'Replay buffer size: {ptr_location}')\n",
    "            print(f'Diffusion buffer size: {agent.diffusion_buffer.ptr}')\n",
    "\n",
    "    # End of epoch wrap-up.\n",
    "    if (t + 1) % steps_per_epoch == 0:\n",
    "        epoch = t // steps_per_epoch\n",
    "\n",
    "        # Test the performance of the deterministic version of the agent.\n",
    "        returns = test_agent(agent, test_env, max_ep_len, logger, n_evals_per_epoch)  # add logging here\n",
    "        if evaluate_bias:\n",
    "            log_bias_evaluation(bias_eval_env, agent, logger, max_ep_len, alpha, gamma, n_mc_eval, n_mc_cutoff)\n",
    "\n",
    "        # reseed should improve reproducibility (should make results the same whether bias evaluation is on or not)\n",
    "        if reseed_each_epoch:\n",
    "            seed_all(epoch)\n",
    "\n",
    "        \"\"\" Logging. \"\"\"\n",
    "        logger.log_tabular('Epoch', epoch)\n",
    "        logger.log_tabular('TotalEnvInteracts', t)\n",
    "        logger.log_tabular('Time', time.time() - start_time)\n",
    "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('EpLen', average_only=True)\n",
    "        logger.log_tabular('TestEpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('TestEpLen', average_only=True)\n",
    "        logger.log_tabular('Q1Vals', with_min_and_max=True)\n",
    "        logger.log_tabular('LossQ1', average_only=True)\n",
    "        logger.log_tabular('LogPi', with_min_and_max=True)\n",
    "        logger.log_tabular('LossPi', average_only=True)\n",
    "        logger.log_tabular('Alpha', with_min_and_max=True)\n",
    "        logger.log_tabular('LossAlpha', average_only=True)\n",
    "        logger.log_tabular('PreTanh', with_min_and_max=True)\n",
    "\n",
    "        if evaluate_bias:\n",
    "            logger.log_tabular(\"MCDisRet\", with_min_and_max=True)\n",
    "            logger.log_tabular(\"MCDisRetEnt\", with_min_and_max=True)\n",
    "            logger.log_tabular(\"QPred\", with_min_and_max=True)\n",
    "            logger.log_tabular(\"QBias\", with_min_and_max=True)\n",
    "            logger.log_tabular(\"QBiasAbs\", with_min_and_max=True)\n",
    "            logger.log_tabular(\"NormQBias\", with_min_and_max=True)\n",
    "            logger.log_tabular(\"QBiasSqr\", with_min_and_max=True)\n",
    "            logger.log_tabular(\"NormQBiasSqr\", with_min_and_max=True)\n",
    "\n",
    "        \"\"\" W&B \"\"\"\n",
    "        wandb.log(logger.log_current_row, step=t)\n",
    "        logger.dump_tabular()\n",
    "\n",
    "        sys.stdout.flush()  # flush logged information to disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
