{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import composuite\n",
    "from diffusion.utils import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def compute_error(agent_dataset, synthetic_dataset):\n",
    "\n",
    "    num_samples = 1000\n",
    "    \n",
    "    agent_observations = agent_dataset['observations']\n",
    "    random_indices = np.random.choice(agent_observations.shape[0], num_samples, replace=False)\n",
    "    sampled_agent_observations = agent_observations[random_indices]\n",
    "\n",
    "    synthetic_observations = synthetic_dataset['observations']\n",
    "    random_indices = np.random.choice(synthetic_observations.shape[0], num_samples, replace=False)\n",
    "    sampled_synthetic_observations = synthetic_observations[random_indices]\n",
    "\n",
    "    mean_agent = np.mean(sampled_agent_observations, axis=0)\n",
    "    mean_synthetic = np.mean(sampled_synthetic_observations, axis=0)\n",
    "\n",
    "    error = np.linalg.norm(mean_agent - mean_synthetic)\n",
    "    \n",
    "    return error\n",
    "\n",
    "\n",
    "def wasserstein_distance(agent_dataset, synthetic_dataset, num_samples=1000):\n",
    "    \n",
    "    agent_observations = agent_dataset['observations']\n",
    "    random_indices = np.random.choice(agent_observations.shape[0], num_samples, replace=False)\n",
    "    sampled_agent_observations = agent_observations[random_indices]\n",
    "\n",
    "    synthetic_observations = synthetic_dataset['observations']\n",
    "    random_indices = np.random.choice(synthetic_observations.shape[0], num_samples, replace=False)\n",
    "    sampled_synthetic_observations = synthetic_observations[random_indices]\n",
    "    \n",
    "    n_dims = sampled_agent_observations.shape[1]\n",
    "    w_distances = []\n",
    "    \n",
    "    for dim in range(n_dims):\n",
    "        w_dist = stats.wasserstein_distance(sampled_agent_observations[:, dim], sampled_synthetic_observations[:, dim])\n",
    "        w_distances.append(w_dist)\n",
    "    \n",
    "    return np.mean(w_distances), w_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = 'expert'\n",
    "base_agent_data_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/data'\n",
    "base_synthetic_data_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/cluster_results/diffusion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_env = composuite.make('IIWA', 'Plate', 'ObjectWall', 'Push', use_task_id_obs=True, ignore_done=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = ['cond_diff_17', 'cond_diff_7', 'cond_diff_8', \n",
    "        'cond_diff_10', 'cond_diff_15', 'cond_diff_19', \n",
    "        'cond_diff_22', 'cond_diff_18', 'cond_diff_23', \n",
    "        'cond_diff_20', 'cond_diff_24', 'cond_diff_25', \n",
    "        'cond_diff_21']\n",
    "num_train_tasks = [16, 32, 48, 64, 80, 96, 112, 128, 144, 176, 192, 208, 244]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run = 'cond_diff_17' \n",
    "\n",
    "# train_tasks = [\n",
    "#     task for task in os.listdir(os.path.join(base_synthetic_data_path, run, 'train'))\n",
    "#     if not task.startswith('.')\n",
    "# ]\n",
    "\n",
    "# task = train_tasks[0]\n",
    "\n",
    "# robot, obj, obst, subtask = task.split('_')\n",
    "\n",
    "# agent_dataset = load_single_composuite_dataset(base_path=base_agent_data_path, \n",
    "#                                                 dataset_type='expert', \n",
    "#                                                 robot=robot, obj=obj, \n",
    "#                                                 obst=obst, task=subtask)\n",
    "# agent_dataset = transitions_dataset(agent_dataset)\n",
    "# agent_dataset, _ = remove_indicator_vectors(representative_env.modality_dims, agent_dataset)\n",
    "\n",
    "# synthetic_dataset = load_single_synthetic_dataset(base_path=os.path.join(base_synthetic_data_path, run, 'train'),\n",
    "#                                                 robot=robot, obj=obj, \n",
    "#                                                 obst=obst, task=subtask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean_dist, w_dists = wasserstein_distance(agent_dataset, synthetic_dataset, num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_error_means = []\n",
    "all_train_error_stds = []\n",
    "all_test_error_means = []\n",
    "all_test_error_stds = []\n",
    "\n",
    "for run in tqdm(runs, desc='Run'):\n",
    "    train_tasks = [\n",
    "        task for task in os.listdir(os.path.join(base_synthetic_data_path, run, 'train'))\n",
    "        if not task.startswith('.')\n",
    "    ]\n",
    "    errors = []\n",
    "    for task in train_tasks:\n",
    "        robot, obj, obst, subtask = task.split('_')\n",
    "        agent_dataset = load_single_composuite_dataset(base_path=base_agent_data_path, \n",
    "                                                       dataset_type='expert', \n",
    "                                                       robot=robot, obj=obj, \n",
    "                                                       obst=obst, task=subtask)\n",
    "        agent_dataset = transitions_dataset(agent_dataset)\n",
    "        agent_dataset, _ = remove_indicator_vectors(representative_env.modality_dims, agent_dataset)\n",
    "\n",
    "        synthetic_dataset = load_single_synthetic_dataset(base_path=os.path.join(base_synthetic_data_path, run, 'train'),\n",
    "                                                          robot=robot, obj=obj, \n",
    "                                                          obst=obst, task=subtask)\n",
    "        # errors.append(compute_error(agent_dataset, synthetic_dataset))\n",
    "        mean_wasserstein_distance, _ = wasserstein_distance(agent_dataset, synthetic_dataset)\n",
    "        errors.append(mean_wasserstein_distance)\n",
    "    all_train_error_means.append(np.mean(errors))\n",
    "    all_train_error_stds.append(np.std(errors))\n",
    "\n",
    "    test_tasks = [\n",
    "        task for task in os.listdir(os.path.join(base_synthetic_data_path, run, 'test'))\n",
    "        if not task.startswith('.')\n",
    "    ]\n",
    "    errors = []\n",
    "    for task in test_tasks:\n",
    "        robot, obj, obst, subtask = task.split('_')\n",
    "        agent_dataset = load_single_composuite_dataset(base_path=base_agent_data_path, \n",
    "                                                       dataset_type='expert', \n",
    "                                                       robot=robot, obj=obj, \n",
    "                                                       obst=obst, task=subtask)\n",
    "        agent_dataset = transitions_dataset(agent_dataset)\n",
    "        agent_dataset, _ = remove_indicator_vectors(representative_env.modality_dims, agent_dataset)\n",
    "\n",
    "        synthetic_dataset = load_single_synthetic_dataset(base_path=os.path.join(base_synthetic_data_path, run, 'test'),\n",
    "                                                        robot=robot, obj=obj, \n",
    "                                                        obst=obst, task=subtask)\n",
    "        # errors.append(compute_error(agent_dataset, synthetic_dataset))\n",
    "        mean_wasserstein_distance, _ = wasserstein_distance(agent_dataset, synthetic_dataset)\n",
    "        errors.append(mean_wasserstein_distance)\n",
    "    all_test_error_means.append(np.mean(errors))\n",
    "    all_test_error_stds.append(np.std(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_means = np.array(all_train_error_means)\n",
    "train_stds = np.array(all_train_error_stds)\n",
    "test_means = np.array(all_test_error_means)\n",
    "test_stds = np.array(all_test_error_stds)\n",
    "num_tasks = np.array(num_train_tasks)\n",
    "\n",
    "num_tasks = num_tasks[:10]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(num_tasks, train_means, label='Train Error', marker='o', linestyle='-', color='blue')\n",
    "plt.plot(num_tasks, test_means, label='Test Error', marker='s', linestyle='-', color='orange')\n",
    "\n",
    "plt.fill_between(\n",
    "    num_tasks, \n",
    "    train_means - train_stds, \n",
    "    train_means + train_stds, \n",
    "    color='blue', alpha=0.4\n",
    ")\n",
    "plt.fill_between(\n",
    "    num_tasks, \n",
    "    test_means - test_stds, \n",
    "    test_means + test_stds, \n",
    "    color='orange', alpha=0.4\n",
    ")\n",
    "\n",
    "plt.xlabel('Number of Training Tasks', fontsize=14)\n",
    "plt.ylabel('Wasserstein Distance', fontsize=14)\n",
    "plt.title('Diffusion Model Generalization', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('wasserstein_dist_diffusion_generalization.pdf', format='pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
