{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import composuite\n",
    "from diffusion.utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "\n",
    "def combine_datasets(dataset_1, dataset_2):\n",
    "\n",
    "    expected_keys = ['observations', 'actions', 'next_observations', 'rewards', 'terminals']\n",
    "    combined_dataset = {}\n",
    "    \n",
    "    for key in expected_keys:\n",
    "        dataset_1_shape = dataset_1[key].shape[1:]\n",
    "        dataset_2_shape = dataset_2[key].shape[1:]\n",
    "        \n",
    "        if dataset_1_shape != dataset_2_shape:\n",
    "            raise ValueError(f\"Shape mismatch for {key}: Expert shape {dataset_1_shape} != Medium shape {dataset_2_shape}\")\n",
    "        \n",
    "        combined_dataset[key] = np.concatenate([\n",
    "            dataset_1[key],\n",
    "            dataset_2[key]\n",
    "        ], axis=0)\n",
    "        \n",
    "        print(f\"Combined shape for {key}: {combined_dataset[key].shape}\")\n",
    "    \n",
    "    return combined_dataset\n",
    "\n",
    "\n",
    "def identify_special_dimensions(data):\n",
    "\n",
    "    integer_dims = []\n",
    "    constant_dims = []\n",
    "    \n",
    "    for i in range(data.shape[1]):\n",
    "        column = data[:, i]\n",
    "        if np.all(np.equal(column, np.round(column))):\n",
    "            integer_dims.append(i)\n",
    "        elif np.all(column == column[0]):\n",
    "            constant_dims.append(i)\n",
    "    \n",
    "    return integer_dims, constant_dims\n",
    "\n",
    "\n",
    "def process_special_dimensions(synthetic_dataset, integer_dims, constant_dims):\n",
    "\n",
    "    processed_dataset = {k: v.copy() for k, v in synthetic_dataset.items()}\n",
    "    \n",
    "    for key in ['observations', 'next_observations']:\n",
    "        # Round integer dimensions\n",
    "        if integer_dims:\n",
    "            processed_dataset[key][:, integer_dims] = np.round(\n",
    "                synthetic_dataset[key][:, integer_dims]\n",
    "            )\n",
    "        \n",
    "        # Round constant dimensions to 1 decimal place\n",
    "        if constant_dims:\n",
    "            processed_dataset[key][:, constant_dims] = np.round(\n",
    "                synthetic_dataset[key][:, constant_dims], \n",
    "                decimals=1\n",
    "            )\n",
    "    \n",
    "    return processed_dataset\n",
    "\n",
    "\n",
    "def create_dimension_labels(state_dims):\n",
    "    \"\"\"Create mapping from dimension index to state meaning.\"\"\"\n",
    "    dim_to_label = {}\n",
    "    current_dim = 0\n",
    "    \n",
    "    for state_type, dims in state_dims.items():\n",
    "        n_dims = dims[0]  # Extract integer from tuple\n",
    "        for i in range(n_dims):\n",
    "            dim_to_label[current_dim + i] = f\"{state_type}_{i}\"\n",
    "        current_dim += n_dims\n",
    "    \n",
    "    return dim_to_label\n",
    "\n",
    "\n",
    "def compute_likelihoods(model, dataloader):\n",
    "\n",
    "    criterion = nn.GaussianNLLLoss()\n",
    "    likelihoods = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for obs, acts, next_obs in dataloader:\n",
    "            mean, log_var = model(obs, acts)\n",
    "            var = torch.exp(log_var)\n",
    "            nll = criterion(mean, next_obs, var).cpu().numpy()\n",
    "            likelihoods.append(-nll)\n",
    "\n",
    "    return np.array(likelihoods)\n",
    "\n",
    "\n",
    "def compute_dimensional_likelihoods(model, dataloader):\n",
    "    \"\"\"Compute per-dimension negative log likelihoods.\n",
    "    Returns:\n",
    "        all_dim_nlls: Array of shape (n_samples, n_dims) containing per-dimension NLL\n",
    "    \"\"\"\n",
    "    all_dim_nlls = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for obs, acts, next_obs in dataloader:\n",
    "            mean, log_var = model(obs, acts)\n",
    "            var = torch.exp(log_var)\n",
    "            # Compute per-dimension NLL\n",
    "            # NLL = 0.5 * (log(2π) + log(σ²) + (x-μ)²/σ²)\n",
    "            sq_mahalanobis = (next_obs - mean)**2 / var\n",
    "            log_det = log_var\n",
    "            dim_nll = 0.5 * (np.log(2 * np.pi) + log_det + sq_mahalanobis)\n",
    "            all_dim_nlls.append(dim_nll.cpu().numpy())\n",
    "    \n",
    "    all_dim_nlls = np.concatenate(all_dim_nlls, axis=0)\n",
    "\n",
    "    return all_dim_nlls\n",
    "\n",
    "\n",
    "def analyze_dimensional_differences(train_dim_nlls, test_dim_nlls, model, dim_to_label):\n",
    "    \"\"\" Analyze which dimensions contribute most to likelihood differences. \"\"\"\n",
    "    \n",
    "    train_means = np.mean(train_dim_nlls, axis=0)\n",
    "    test_means = np.mean(test_dim_nlls, axis=0)\n",
    "    differences = test_means - train_means\n",
    "    \n",
    "    worst_dims = np.argsort(differences)[::-1]\n",
    "    \n",
    "    summary = []\n",
    "    for dim in worst_dims:\n",
    "        summary.append({\n",
    "            'dimension': dim_to_label[dim],\n",
    "            'dim_index': dim,\n",
    "            'train_nll': train_means[dim],\n",
    "            'test_nll': test_means[dim],\n",
    "            'difference': differences[dim]\n",
    "        })\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def visualize_dimensional_differences(train_dim_nlls, test_dim_nlls, model, dim_to_label):\n",
    "    \"\"\"Visualize per-dimension likelihood differences between train and test data.\"\"\"\n",
    "    \n",
    "    train_means = np.mean(train_dim_nlls, axis=0)\n",
    "    test_means = np.mean(test_dim_nlls, axis=0)\n",
    "    differences = test_means - train_means\n",
    "    \n",
    "    worst_dims_idx = np.argsort(differences)[::-1]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), height_ratios=[1.5, 1])\n",
    "    \n",
    "    n_dims = len(worst_dims_idx)\n",
    "    top_k = min(10, n_dims)  # Show top 10 worst dimensions\n",
    "    worst_dims = worst_dims_idx[:top_k]\n",
    "    \n",
    "    plot_data = []\n",
    "    labels = []\n",
    "    for dim in worst_dims:\n",
    "        plot_data.extend([train_dim_nlls[:, dim], test_dim_nlls[:, dim]])\n",
    "        labels.extend([f'Train {dim_to_label[dim]}', f'Test {dim_to_label[dim]}'])\n",
    "    \n",
    "    sns.boxplot(data=plot_data, ax=ax1)\n",
    "    ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax1.set_title('Distribution of Negative Log-Likelihoods by Dimension\\n(Top 10 Most Different Dimensions)')\n",
    "    ax1.set_ylabel('Negative Log-Likelihood')\n",
    "    \n",
    "    dims = np.arange(len(worst_dims))\n",
    "    diffs = differences[worst_dims]\n",
    "    \n",
    "    bars = ax2.bar(dims, diffs)\n",
    "    \n",
    "    colors = plt.cm.RdYlBu_r(np.linspace(0, 1, len(bars)))\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    ax2.set_xticks(dims)\n",
    "    ax2.set_xticklabels([dim_to_label[d] for d in worst_dims], rotation=45, ha='right')\n",
    "    ax2.set_title('Difference in Mean NLL (Test - Train)\\nLarger Values = Worse Synthetic Data')\n",
    "    ax2.set_ylabel('Difference in NLL')\n",
    "    \n",
    "    textstr = '\\n'.join([\n",
    "        'Summary Statistics:',\n",
    "        f'Max Difference: {np.max(differences):.3f}',\n",
    "        f'Mean Difference: {np.mean(differences):.3f}',\n",
    "        f'Median Difference: {np.median(differences):.3f}'\n",
    "    ])\n",
    "    \n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax2.text(0.02, 0.98, textstr, transform=ax2.transAxes, fontsize=9,\n",
    "             verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return [dim_to_label[dim] for dim in worst_dims], differences[worst_dims]\n",
    "\n",
    "\n",
    "def visualize_state_space_differences(train_dim_nlls, test_dim_nlls, model, dim_to_label):\n",
    "    \"\"\"Visualize per-dimension likelihood differences with state space labels.\"\"\"\n",
    "    \n",
    "    train_means = np.mean(train_dim_nlls, axis=0)\n",
    "    test_means = np.mean(test_dim_nlls, axis=0)\n",
    "    differences = test_means - train_means\n",
    "    \n",
    "    worst_dims_idx = np.argsort(differences)[::-1]\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), height_ratios=[1.5, 1])\n",
    "    \n",
    "    # 1. Box plot for top k worst dimensions\n",
    "    n_dims = len(worst_dims_idx)\n",
    "    top_k = min(15, n_dims)  # Show top 15 worst dimensions\n",
    "    worst_dims = worst_dims_idx[:top_k]\n",
    "    \n",
    "    plot_data = []\n",
    "    labels = []\n",
    "    for dim in worst_dims:\n",
    "        plot_data.extend([train_dim_nlls[:, dim], test_dim_nlls[:, dim]])\n",
    "        labels.extend([f'Train\\n{dim_to_label[dim]}', f'Test\\n{dim_to_label[dim]}'])\n",
    "    \n",
    "    sns.boxplot(data=plot_data, ax=ax1)\n",
    "    ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax1.set_title('Distribution of Negative Log-Likelihoods by State Dimension\\n(Top 15 Most Different Dimensions)')\n",
    "    ax1.set_ylabel('Negative Log-Likelihood')\n",
    "    \n",
    "    # Group dimensions by state type\n",
    "    state_type_dims = {}\n",
    "    for dim, label in dim_to_label.items():\n",
    "        state_type = label.split('_')[0]  # Extract state type from label\n",
    "        if state_type not in state_type_dims:\n",
    "            state_type_dims[state_type] = []\n",
    "        state_type_dims[state_type].append(dim)\n",
    "    \n",
    "    # Calculate statistics for each state type\n",
    "    state_type_diffs = {}\n",
    "    for state_type, dims in state_type_dims.items():\n",
    "        state_diffs = differences[dims]\n",
    "        state_type_diffs[state_type] = {\n",
    "            'mean': np.mean(state_diffs),\n",
    "            'max': np.max(state_diffs),\n",
    "            'worst_dim': dims[np.argmax(state_diffs)],\n",
    "            'all_diffs': state_diffs\n",
    "        }\n",
    "    \n",
    "    state_types = list(state_type_dims.keys())\n",
    "    means = [state_type_diffs[st]['mean'] for st in state_types]\n",
    "    maxes = [state_type_diffs[st]['max'] for st in state_types]\n",
    "    \n",
    "    x = np.arange(len(state_types))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax2.bar(x - width/2, means, width, label='Mean Difference')\n",
    "    ax2.bar(x + width/2, maxes, width, label='Max Difference')\n",
    "    \n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([st.replace('-state', '') for st in state_types], rotation=45, ha='right')\n",
    "    ax2.set_title('NLL Differences by State Type (Test - Train)')\n",
    "    ax2.set_ylabel('Difference in NLL')\n",
    "    ax2.legend()\n",
    "    \n",
    "    textstr = ['State Space Analysis:']\n",
    "    for state_type in state_types:\n",
    "        stats = state_type_diffs[state_type]\n",
    "        worst_dim_label = dim_to_label[stats['worst_dim']]\n",
    "        textstr.extend([\n",
    "            f'\\n{state_type}:',\n",
    "            f'  Mean Diff: {stats[\"mean\"]:.3f}',\n",
    "            f'  Max Diff: {stats[\"max\"]:.3f}',\n",
    "            f'  Worst Dim: {worst_dim_label}'\n",
    "        ])\n",
    "    \n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax2.text(1.02, 0.98, '\\n'.join(textstr), transform=ax2.transAxes, fontsize=9,\n",
    "             verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return state_type_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_agent_data_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/data'\n",
    "base_synthetic_data_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/cluster_results/diffusion/cond_diff_20/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = 'IIWA'\n",
    "obj = 'Dumbbell'\n",
    "obst = 'ObjectDoor'\n",
    "subtask = 'Trashcan'\n",
    "\n",
    "representative_indicators_env = composuite.make(robot, obj, obst, subtask, use_task_id_obs=True, ignore_done=False)\n",
    "modality_dims = representative_indicators_env.modality_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_agent_dataset = load_single_composuite_dataset(base_path=base_agent_data_path, \n",
    "                                                      dataset_type='medium', \n",
    "                                                      robot=robot, obj=obj, \n",
    "                                                      obst=obst, task=subtask)\n",
    "medium_agent_dataset = transitions_dataset(medium_agent_dataset)\n",
    "medium_agent_dataset, _ = remove_indicator_vectors(modality_dims, medium_agent_dataset)\n",
    "\n",
    "warmstart_agent_dataset = load_single_composuite_dataset(base_path=base_agent_data_path, \n",
    "                                                      dataset_type='warmstart', \n",
    "                                                      robot=robot, obj=obj, \n",
    "                                                      obst=obst, task=subtask)\n",
    "warmstart_agent_dataset = transitions_dataset(warmstart_agent_dataset)\n",
    "warmstart_agent_dataset, _ = remove_indicator_vectors(modality_dims, warmstart_agent_dataset)\n",
    "\n",
    "expert_agent_dataset = load_single_composuite_dataset(base_path=base_agent_data_path, \n",
    "                                                      dataset_type='expert', \n",
    "                                                      robot=robot, obj=obj, \n",
    "                                                      obst=obst, task=subtask)\n",
    "expert_agent_dataset = transitions_dataset(expert_agent_dataset)\n",
    "expert_agent_dataset, _ = remove_indicator_vectors(modality_dims, expert_agent_dataset)\n",
    "\n",
    "\n",
    "print(medium_agent_dataset['observations'].shape, \n",
    "      warmstart_agent_dataset['observations'].shape, \n",
    "      expert_agent_dataset['observations'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dataset = combine_datasets(medium_agent_dataset, expert_agent_dataset)\n",
    "agent_dataset = combine_datasets(agent_dataset, warmstart_agent_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset = load_single_synthetic_dataset(base_path=base_synthetic_data_path, \n",
    "                                                  robot=robot, obj=obj, \n",
    "                                                  obst=obst, task=subtask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_dims, constant_dims = identify_special_dimensions(agent_dataset['observations'])\n",
    "print('Integer dimensions:', integer_dims)\n",
    "print('Constant dimensions:', constant_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset = process_special_dimensions(synthetic_dataset, integer_dims, constant_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_dataset['observations'].shape, synthetic_dataset['observations'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self, data, eps=1e-8):\n",
    "        self.mean = np.mean(data, axis=0)\n",
    "        self.std = np.std(data, axis=0) + eps\n",
    "\n",
    "    def normalize(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def denormalize(self, normalized_data):\n",
    "        return normalized_data * self.std + self.mean\n",
    "\n",
    "\n",
    "def prepare_train_data(agent_dataset, split_ratio=0.9):\n",
    "\n",
    "    obs_normalizer = Normalizer(agent_dataset['observations'])\n",
    "    act_normalizer = Normalizer(agent_dataset['actions'])\n",
    "    \n",
    "    norm_obs = obs_normalizer.normalize(agent_dataset['observations'])\n",
    "    norm_acts = act_normalizer.normalize(agent_dataset['actions'])\n",
    "    norm_next_obs = obs_normalizer.normalize(agent_dataset['next_observations'])\n",
    "    \n",
    "    obs = torch.tensor(norm_obs, dtype=torch.float32)\n",
    "    acts = torch.tensor(norm_acts, dtype=torch.float32)\n",
    "    next_obs = torch.tensor(norm_next_obs, dtype=torch.float32)\n",
    "    \n",
    "    dataset = TensorDataset(obs, acts, next_obs)\n",
    "    train_size = int(split_ratio * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    return (DataLoader(train_data, batch_size=512, shuffle=True), \n",
    "            DataLoader(val_data, batch_size=512, shuffle=False),\n",
    "            obs_normalizer, act_normalizer)\n",
    "\n",
    "\n",
    "def prepare_test_data(synthetic_dataset, obs_normalizer, act_normalizer):\n",
    "\n",
    "    norm_obs = obs_normalizer.normalize(synthetic_dataset['observations'])\n",
    "    norm_acts = act_normalizer.normalize(synthetic_dataset['actions'])\n",
    "    norm_next_obs = obs_normalizer.normalize(synthetic_dataset['next_observations'])\n",
    "    \n",
    "    obs = torch.tensor(norm_obs, dtype=torch.float32)\n",
    "    acts = torch.tensor(norm_acts, dtype=torch.float32)\n",
    "    next_obs = torch.tensor(norm_next_obs, dtype=torch.float32)\n",
    "    \n",
    "    dataset = TensorDataset(obs, acts, next_obs)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "\n",
    "class ProbabilisticDynamicsModel(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, obs_normalizer, act_normalizer, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.obs_normalizer = obs_normalizer\n",
    "        self.act_normalizer = act_normalizer\n",
    "\n",
    "        self.shared_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.mean_head = nn.Linear(256, obs_dim)\n",
    "        self.log_var_head = nn.Linear(256, obs_dim)\n",
    "        self.log_var_scale = nn.Parameter(torch.ones(obs_dim) * 1.0)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
    "            module.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        x = self.shared_net(x)\n",
    "        mean = self.mean_head(x)\n",
    "        log_var = self.log_var_scale * torch.tanh(self.log_var_head(x))\n",
    "        return mean, log_var\n",
    "    \n",
    "    def predict(self, obs, act):\n",
    "        \"\"\" Make predictions in the original (unnormalized) space. \"\"\"\n",
    "\n",
    "        norm_obs = torch.tensor(self.obs_normalizer.normalize(obs), dtype=torch.float32)\n",
    "        norm_acts = torch.tensor(self.act_normalizer.normalize(act), dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            norm_mean, norm_log_var = self(norm_obs, norm_acts)\n",
    "        \n",
    "        mean = self.obs_normalizer.denormalize(norm_mean.numpy())\n",
    "        log_var = norm_log_var.numpy() + 2 * np.log(self.obs_normalizer.std)\n",
    "        \n",
    "        return mean, log_var\n",
    "    \n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=20, lr=3e-4, patience=5):\n",
    "\n",
    "    criterion = nn.GaussianNLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for obs, acts, next_obs in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            mean, log_var = model(obs, acts)\n",
    "            var = torch.exp(log_var)\n",
    "            loss = criterion(mean, next_obs, var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for obs, acts, next_obs in val_loader:\n",
    "                mean, log_var = model(obs, acts)\n",
    "                var = torch.exp(log_var)\n",
    "                loss = criterion(mean, next_obs, var)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = composuite.make(robot, obj, obst, subtask, use_task_id_obs=False, ignore_done=False)\n",
    "dim_to_label = create_dimension_labels(env.modality_dims)\n",
    "\n",
    "train_loader, val_loader, obs_normalizer, act_normalizer = prepare_train_data(agent_dataset)\n",
    "test_loader = prepare_test_data(synthetic_dataset, obs_normalizer, act_normalizer)\n",
    "\n",
    "model = ProbabilisticDynamicsModel(\n",
    "    obs_dim=env.obs_dim, \n",
    "    act_dim=env.action_dim,\n",
    "    obs_normalizer=obs_normalizer,\n",
    "    act_normalizer=act_normalizer\n",
    ")\n",
    "\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_likelihoods = compute_likelihoods(model, train_loader)\n",
    "val_likelihoods = compute_likelihoods(model, val_loader)\n",
    "test_likelihoods = compute_likelihoods(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(map(np.min, [train_likelihoods, val_likelihoods])), \n",
    "                   max(map(np.max, [train_likelihoods, val_likelihoods])), \n",
    "                   15)\n",
    "\n",
    "plt.hist(train_likelihoods, bins=bins, alpha=0.5, label='Train Set')\n",
    "plt.hist(val_likelihoods, bins=bins, alpha=0.5, label='Val. Set')\n",
    "\n",
    "plt.xlabel('Log Likelihood')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(map(np.min, [train_likelihoods, test_likelihoods])), \n",
    "                   max(map(np.max, [train_likelihoods, test_likelihoods])), \n",
    "                   15)\n",
    "\n",
    "plt.hist(train_likelihoods, bins=bins, alpha=0.5, label='Train Set')\n",
    "plt.hist(test_likelihoods, bins=bins, alpha=0.5, label='Test Set')\n",
    "\n",
    "plt.xlabel('Log Likelihood')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(min(map(np.min, [train_likelihoods, val_likelihoods, test_likelihoods])), \n",
    "                   max(map(np.max, [train_likelihoods, val_likelihoods, test_likelihoods])), \n",
    "                   15)\n",
    "\n",
    "plt.hist(train_likelihoods, bins=bins, alpha=0.5, label='Train Set')\n",
    "plt.hist(val_likelihoods, bins=bins, alpha=0.5, label='Val. Set')\n",
    "plt.hist(test_likelihoods, bins=bins, alpha=0.5, label='Test Set')\n",
    "\n",
    "plt.xlabel('Log Likelihood')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dim_nlls = compute_dimensional_likelihoods(model, train_loader)\n",
    "test_dim_nlls = compute_dimensional_likelihoods(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = analyze_dimensional_differences(train_dim_nlls, test_dim_nlls, model, dim_to_label)\n",
    "\n",
    "print(\"\\nDimensions sorted by likelihood difference (worst to best):\")\n",
    "for entry in summary:\n",
    "    print(f\"Dimension {entry['dimension']}: \"\n",
    "          f\"Train NLL = {entry['train_nll']:6.3f}, \"\n",
    "          f\"Test NLL = {entry['test_nll']:6.3f}, \"\n",
    "          f\"Diff = {entry['difference']:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_dims, diffs = visualize_dimensional_differences(train_dim_nlls, test_dim_nlls, model, dim_to_label)\n",
    "\n",
    "print(\"\\nTop differences by dimension:\")\n",
    "for dim, diff in zip(worst_dims, diffs):\n",
    "    print(f\"Dimension {dim}: {diff:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_diffs = visualize_state_space_differences(train_dim_nlls, test_dim_nlls, model, dim_to_label)\n",
    "\n",
    "print(\"\\nAnalysis by State Type:\")\n",
    "for state_type, stats in state_diffs.items():\n",
    "    print(f\"\\n{state_type}:\")\n",
    "    print(f\"  Mean Difference: {stats['mean']:.3f}\")\n",
    "    print(f\"  Max Difference: {stats['max']:.3f}\")\n",
    "    print(f\"  Worst Dimension: {stats['worst_dim']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
