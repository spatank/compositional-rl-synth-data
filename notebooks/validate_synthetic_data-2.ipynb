{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import composuite\n",
    "from diffusion.utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "\n",
    "def combine_datasets(dataset_1, dataset_2):\n",
    "\n",
    "    expected_keys = ['observations', 'actions', 'next_observations', 'rewards', 'terminals']\n",
    "    combined_dataset = {}\n",
    "    \n",
    "    for key in expected_keys:\n",
    "        dataset_1_shape = dataset_1[key].shape[1:]\n",
    "        dataset_2_shape = dataset_2[key].shape[1:]\n",
    "        \n",
    "        if dataset_1_shape != dataset_2_shape:\n",
    "            raise ValueError(f\"Shape mismatch for {key}: Expert shape {dataset_1_shape} != Medium shape {dataset_2_shape}\")\n",
    "        \n",
    "        combined_dataset[key] = np.concatenate([\n",
    "            dataset_1[key],\n",
    "            dataset_2[key]\n",
    "        ], axis=0)\n",
    "        \n",
    "        print(f\"Combined shape for {key}: {combined_dataset[key].shape}\")\n",
    "    \n",
    "    return combined_dataset\n",
    "\n",
    "\n",
    "def identify_special_dimensions(data):\n",
    "\n",
    "    integer_dims = []\n",
    "    constant_dims = []\n",
    "    \n",
    "    for i in range(data.shape[1]):\n",
    "        column = data[:, i]\n",
    "        if np.all(np.equal(column, np.round(column))):\n",
    "            integer_dims.append(i)\n",
    "        elif np.all(column == column[0]):\n",
    "            constant_dims.append(i)\n",
    "    \n",
    "    return integer_dims, constant_dims\n",
    "\n",
    "\n",
    "def process_special_dimensions(synthetic_dataset, integer_dims, constant_dims):\n",
    "\n",
    "    processed_dataset = {k: v.copy() for k, v in synthetic_dataset.items()}\n",
    "    \n",
    "    for key in ['observations', 'next_observations']:\n",
    "        # Round integer dimensions\n",
    "        if integer_dims:\n",
    "            processed_dataset[key][:, integer_dims] = np.round(\n",
    "                synthetic_dataset[key][:, integer_dims]\n",
    "            )\n",
    "        \n",
    "        # Round constant dimensions to 1 decimal place\n",
    "        if constant_dims:\n",
    "            processed_dataset[key][:, constant_dims] = np.round(\n",
    "                synthetic_dataset[key][:, constant_dims], \n",
    "                decimals=1\n",
    "            )\n",
    "    \n",
    "    return processed_dataset\n",
    "\n",
    "\n",
    "def create_dimension_labels(state_dims):\n",
    "    \"\"\"Create mapping from dimension index to state meaning.\"\"\"\n",
    "    dim_to_label = {}\n",
    "    current_dim = 0\n",
    "    \n",
    "    for state_type, dims in state_dims.items():\n",
    "        n_dims = dims[0]  # Extract integer from tuple\n",
    "        for i in range(n_dims):\n",
    "            dim_to_label[current_dim + i] = f\"{state_type}_{i}\"\n",
    "        current_dim += n_dims\n",
    "    \n",
    "    return dim_to_label\n",
    "\n",
    "\n",
    "def compute_mse_errors(model, dataloader):\n",
    "    \"\"\"Compute MSE between predicted and actual next states.\"\"\"\n",
    "    criterion = nn.MSELoss(reduction='none')\n",
    "    all_errors = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for obs, acts, next_obs in dataloader:\n",
    "            next_obs_pred = model(obs, acts)\n",
    "            errors = criterion(next_obs_pred, next_obs)  # Shape: [batch_size, obs_dim]\n",
    "            all_errors.append(errors.cpu().numpy())\n",
    "    \n",
    "    return np.concatenate(all_errors, axis=0)\n",
    "\n",
    "\n",
    "def analyze_dimensional_errors(train_errors, test_errors, model, dim_to_label):\n",
    "    \"\"\"Analyze which dimensions have the highest prediction errors.\"\"\"\n",
    "    train_means = np.mean(train_errors, axis=0)\n",
    "    test_means = np.mean(test_errors, axis=0)\n",
    "    differences = test_means - train_means\n",
    "    \n",
    "    worst_dims = np.argsort(differences)[::-1]\n",
    "    \n",
    "    summary = []\n",
    "    for dim in worst_dims:\n",
    "        summary.append({\n",
    "            'dimension': dim_to_label[dim],\n",
    "            'dim_index': dim,\n",
    "            'train_mse': train_means[dim],\n",
    "            'test_mse': test_means[dim],\n",
    "            'difference': differences[dim]\n",
    "        })\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def visualize_dimensional_errors(train_errors, test_errors, model, dim_to_label):\n",
    "    \"\"\"Visualize per-dimension prediction errors between train and test data.\"\"\"\n",
    "    train_means = np.mean(train_errors, axis=0)\n",
    "    test_means = np.mean(test_errors, axis=0)\n",
    "    differences = test_means - train_means\n",
    "    \n",
    "    worst_dims_idx = np.argsort(differences)[::-1]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), height_ratios=[1.5, 1])\n",
    "    \n",
    "    n_dims = len(worst_dims_idx)\n",
    "    top_k = min(10, n_dims)  # Show top 10 worst dimensions\n",
    "    worst_dims = worst_dims_idx[:top_k]\n",
    "    \n",
    "    plot_data = []\n",
    "    labels = []\n",
    "    for dim in worst_dims:\n",
    "        plot_data.extend([train_errors[:, dim], test_errors[:, dim]])\n",
    "        labels.extend([f'Train {dim_to_label[dim]}', f'Test {dim_to_label[dim]}'])\n",
    "    \n",
    "    sns.boxplot(data=plot_data, ax=ax1)\n",
    "    ax1.set_xticklabels(labels, rotation=45, ha='right')\n",
    "    ax1.set_title('Distribution of MSE Errors by Dimension\\n(Top 10 Most Different Dimensions)')\n",
    "    ax1.set_ylabel('Mean Squared Error')\n",
    "    \n",
    "    dims = np.arange(len(worst_dims))\n",
    "    diffs = differences[worst_dims]\n",
    "    \n",
    "    bars = ax2.bar(dims, diffs)\n",
    "    \n",
    "    colors = plt.cm.RdYlBu_r(np.linspace(0, 1, len(bars)))\n",
    "    for bar, color in zip(bars, colors):\n",
    "        bar.set_color(color)\n",
    "    \n",
    "    ax2.set_xticks(dims)\n",
    "    ax2.set_xticklabels([dim_to_label[d] for d in worst_dims], rotation=45, ha='right')\n",
    "    ax2.set_title('Difference in Mean MSE (Test - Train)\\nLarger Values = Worse Synthetic Data')\n",
    "    ax2.set_ylabel('Difference in MSE')\n",
    "    \n",
    "    textstr = '\\n'.join([\n",
    "        'Summary Statistics:',\n",
    "        f'Max Difference: {np.max(differences):.3f}',\n",
    "        f'Mean Difference: {np.mean(differences):.3f}',\n",
    "        f'Median Difference: {np.median(differences):.3f}'\n",
    "    ])\n",
    "    \n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    ax2.text(0.02, 0.98, textstr, transform=ax2.transAxes, fontsize=9,\n",
    "             verticalalignment='top', bbox=props)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return [dim_to_label[dim] for dim in worst_dims], differences[worst_dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_agent_data_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/data'\n",
    "# base_synthetic_data_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/cluster_results/diffusion/cond_diff_20/train/'\n",
    "base_synthetic_data_path = '/Users/shubhankar/Developer/compositional-rl-synth-data/cluster_results/Quan/180M/128/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = 'IIWA'\n",
    "obj = 'Hollowbox'\n",
    "obst = 'None'\n",
    "subtask = 'Shelf'\n",
    "\n",
    "representative_indicators_env = composuite.make(robot, obj, obst, subtask, use_task_id_obs=True, ignore_done=False)\n",
    "modality_dims = representative_indicators_env.modality_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_agent_dataset = load_single_composuite_dataset(base_path=base_agent_data_path, \n",
    "                                                      dataset_type='medium', \n",
    "                                                      robot=robot, obj=obj, \n",
    "                                                      obst=obst, task=subtask)\n",
    "medium_agent_dataset = transitions_dataset(medium_agent_dataset)\n",
    "medium_agent_dataset, _ = remove_indicator_vectors(modality_dims, medium_agent_dataset)\n",
    "\n",
    "warmstart_agent_dataset = load_single_composuite_dataset(base_path=base_agent_data_path, \n",
    "                                                      dataset_type='warmstart', \n",
    "                                                      robot=robot, obj=obj, \n",
    "                                                      obst=obst, task=subtask)\n",
    "warmstart_agent_dataset = transitions_dataset(warmstart_agent_dataset)\n",
    "warmstart_agent_dataset, _ = remove_indicator_vectors(modality_dims, warmstart_agent_dataset)\n",
    "\n",
    "expert_agent_dataset = load_single_composuite_dataset(base_path=base_agent_data_path, \n",
    "                                                      dataset_type='expert', \n",
    "                                                      robot=robot, obj=obj, \n",
    "                                                      obst=obst, task=subtask)\n",
    "expert_agent_dataset = transitions_dataset(expert_agent_dataset)\n",
    "expert_agent_dataset, _ = remove_indicator_vectors(modality_dims, expert_agent_dataset)\n",
    "\n",
    "\n",
    "print(medium_agent_dataset['observations'].shape, \n",
    "      warmstart_agent_dataset['observations'].shape, \n",
    "      expert_agent_dataset['observations'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_dataset = combine_datasets(medium_agent_dataset, expert_agent_dataset)\n",
    "agent_dataset = combine_datasets(agent_dataset, warmstart_agent_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset = load_single_synthetic_dataset(base_path=base_synthetic_data_path, \n",
    "                                                  robot=robot, obj=obj, \n",
    "                                                  obst=obst, task=subtask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_dims, constant_dims = identify_special_dimensions(agent_dataset['observations'])\n",
    "print('Integer dimensions:', integer_dims)\n",
    "print('Constant dimensions:', constant_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_dataset = process_special_dimensions(synthetic_dataset, integer_dims, constant_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent_dataset['observations'].shape, synthetic_dataset['observations'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self, data, eps=1e-8):\n",
    "        self.mean = np.mean(data, axis=0)\n",
    "        self.std = np.std(data, axis=0) + eps\n",
    "\n",
    "    def normalize(self, data):\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def denormalize(self, normalized_data):\n",
    "        return normalized_data * self.std + self.mean\n",
    "\n",
    "\n",
    "def prepare_train_data(agent_dataset, split_ratio=0.9):\n",
    "\n",
    "    obs_normalizer = Normalizer(agent_dataset['observations'])\n",
    "    act_normalizer = Normalizer(agent_dataset['actions'])\n",
    "    \n",
    "    norm_obs = obs_normalizer.normalize(agent_dataset['observations'])\n",
    "    norm_acts = act_normalizer.normalize(agent_dataset['actions'])\n",
    "    norm_next_obs = obs_normalizer.normalize(agent_dataset['next_observations'])\n",
    "    \n",
    "    obs = torch.tensor(norm_obs, dtype=torch.float32)\n",
    "    acts = torch.tensor(norm_acts, dtype=torch.float32)\n",
    "    next_obs = torch.tensor(norm_next_obs, dtype=torch.float32)\n",
    "    \n",
    "    dataset = TensorDataset(obs, acts, next_obs)\n",
    "    train_size = int(split_ratio * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    return (DataLoader(train_data, batch_size=512, shuffle=True), \n",
    "            DataLoader(val_data, batch_size=512, shuffle=False),\n",
    "            obs_normalizer, act_normalizer)\n",
    "\n",
    "\n",
    "def prepare_test_data(synthetic_dataset, obs_normalizer, act_normalizer):\n",
    "\n",
    "    norm_obs = obs_normalizer.normalize(synthetic_dataset['observations'])\n",
    "    norm_acts = act_normalizer.normalize(synthetic_dataset['actions'])\n",
    "    norm_next_obs = obs_normalizer.normalize(synthetic_dataset['next_observations'])\n",
    "    \n",
    "    obs = torch.tensor(norm_obs, dtype=torch.float32)\n",
    "    acts = torch.tensor(norm_acts, dtype=torch.float32)\n",
    "    next_obs = torch.tensor(norm_next_obs, dtype=torch.float32)\n",
    "    \n",
    "    dataset = TensorDataset(obs, acts, next_obs)\n",
    "\n",
    "    return DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "\n",
    "\n",
    "class DeterministicDynamicsModel(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, obs_normalizer, act_normalizer, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.obs_normalizer = obs_normalizer\n",
    "        self.act_normalizer = act_normalizer\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(256, obs_dim)\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))\n",
    "            module.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        next_obs_pred = self.net(x)\n",
    "        return next_obs_pred\n",
    "    \n",
    "    def predict(self, obs, act):\n",
    "        \"\"\"Make predictions in the original (unnormalized) space.\"\"\"\n",
    "        norm_obs = torch.tensor(self.obs_normalizer.normalize(obs), dtype=torch.float32)\n",
    "        norm_acts = torch.tensor(self.act_normalizer.normalize(act), dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            norm_next_obs = self(norm_obs, norm_acts)\n",
    "        \n",
    "        next_obs = self.obs_normalizer.denormalize(norm_next_obs.numpy())\n",
    "        return next_obs\n",
    "\n",
    "\n",
    "def train_deterministic_model(model, train_loader, val_loader, epochs=20, lr=3e-4, patience=5):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for obs, acts, next_obs in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            next_obs_pred = model(obs, acts)\n",
    "            loss = criterion(next_obs_pred, next_obs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for obs, acts, next_obs in val_loader:\n",
    "                next_obs_pred = model(obs, acts)\n",
    "                loss = criterion(next_obs_pred, next_obs)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = composuite.make(robot, obj, obst, subtask, use_task_id_obs=False, ignore_done=False)\n",
    "dim_to_label = create_dimension_labels(env.modality_dims)\n",
    "\n",
    "train_loader, val_loader, obs_normalizer, act_normalizer = prepare_train_data(agent_dataset)\n",
    "test_loader = prepare_test_data(synthetic_dataset, obs_normalizer, act_normalizer)\n",
    "\n",
    "model = DeterministicDynamicsModel(\n",
    "    obs_dim=env.obs_dim, \n",
    "    act_dim=env.action_dim,\n",
    "    obs_normalizer=obs_normalizer,\n",
    "    act_normalizer=act_normalizer\n",
    ")\n",
    "\n",
    "train_losses, val_losses = train_deterministic_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors = compute_mse_errors(model, train_loader)\n",
    "test_errors = compute_mse_errors(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = analyze_dimensional_errors(train_errors, test_errors, model, dim_to_label)\n",
    "print(\"\\nDimensions sorted by error difference (worst to best):\")\n",
    "for entry in summary[:10]:  # Show top 10\n",
    "    print(f\"Dimension {entry['dimension']}: \"\n",
    "          f\"Train MSE = {entry['train_mse']:6.3f}, \"\n",
    "          f\"Test MSE = {entry['test_mse']:6.3f}, \"\n",
    "          f\"Diff = {entry['difference']:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_dims, diffs = visualize_dimensional_errors(train_errors, test_errors, model, dim_to_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
